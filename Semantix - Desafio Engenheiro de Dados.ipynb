{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Semantix</font>\n",
    "## <font color='blue'>Desafio Engenheiro de Dados</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qual o objetivo do comando <u>cache</u> em Spark?\n",
    "Possibilita a persistencia dos dados em memória, permitindo que sejam reutilizados em etapas seguintes evitando um novo processamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê?\n",
    "O MapReduce utiliza-se do disco (HDFS) para realizar a gravação dos resultados intermediários em uma atividade de processamento, ao passo que o Spark utiliza-se da memória.\n",
    "O processamento em memória é até 100x mais rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qual é a função do <u>SparkContext</u>?\n",
    "Estabelecer a conexão com o ambiente de execução do Spark, permitindo acesso à todas as suas funcionalidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explique com suas palavras  o que é <u>Resilient Distributed Datasets (RDD)</u>.\n",
    "É uma coleção de elementos de dados particionados distribuída e imutável."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>GroupByKey</u> é menos eficiente que <u>reduceByKey</u> em grandes dataset. Por quê?\n",
    "O GroupByKey realiza o agrupamento de todos os dados de diferentes partições para só então realizar a operação.\n",
    "O reduceByKey realiza o agrupamento dos dados por partição para depois realizar a operação. Isso implica em um volume menor de dados trafegando na rede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Explique o que o código Scala abaixo faz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val textFile = sc.textFile(\"hdfs://...\") #CRIA UM RDD A PARTIR DE UM ARQUIVO DO HDFS\n",
    "val counts = textFile.flatMap(line => line.split(\" \") #APLICA O SPLIT PARA QUEBRAR O RDD EM PALAVRAS\n",
    "    .map(word => (word, 1)) #FAZ UM MAPEAMENTO \"CHAVE, VALOR\" DE CADA PALAVRA ATRIBUINDO O VALOR 1\n",
    "    .reduceByKey(_ + _) #FAZ REDUÇÃO SUMARIZANDO OS VALORES POR PALAVRAS, RESULTANDO NA QUANTIDADE TOTAL DE CADA PALAVRA\n",
    "counts.saveAsTextFile(\"hdfs://...\") #SALVA O ARQUIVO COM A CONTAGEM DE REPETIÇÃO DAS PALAVRAS NO HDFS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>HTTP requests to the NASA Kennedy Space Center WWW server</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o Spark Session\n",
    "spSession = SparkSession.builder.master(\"local\").appName(\"Semantix-Desafio\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o SQL Context\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os RDDs\n",
    "Jul95 = sc.textFile(\"access_log_Jul95.csv\")\n",
    "Aug95 = sc.textFile(\"access_log_Aug95.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unindo os RDDs e fazendo o tratamento de limpeza e estruturação\n",
    "nasa1 = Jul95.union(Aug95)\n",
    "nasa2 = nasa1.map(lambda x: x.replace(\" - - [\",\"|\"))\n",
    "nasa3 = nasa2.map(lambda x: x.replace(\" -\",\"|\"))\n",
    "nasa4 = nasa3.map(lambda x: x.replace('] \"',\"|\"))\n",
    "nasa5 = nasa4.map(lambda x: x.replace('\" ',\"|\"))\n",
    "nasa6 = nasa5.map(lambda line: line.split(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando linhas independentes \n",
    "nasaFinal = nasa6.map(lambda p: Row(HOST = str(p[0]), DATA = str(p[1].split(\":\")[0]), REQUISICAO = str(p[3]), RETORNO = int(p[4].split(\" \")[0]), BYTES = int(p[4].split(\" \")[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um Data Frame\n",
    "nasaDF = spSession.createDataFrame(nasaFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma tabela temporária\n",
    "nasaDF.createOrReplaceTempView(\"nasaTB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executando SQL\n",
    "spSession.sql(\"select * from nasaTB\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questões\n",
    "<b>Responda as seguintes questões devem ser desenvolvidas em Spark utilizando a sua linguagem de preferência.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Número de hosts únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. O total de erros 404."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Os 5 URLs que mais causaram erro 404."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Quantidade de erros 404 por dia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. O total de bytes retornados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
